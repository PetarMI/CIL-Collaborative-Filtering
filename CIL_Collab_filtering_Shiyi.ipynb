{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD Based Collaborative Filtering Aloghrithm \n",
    "\n",
    "# for Movie Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Pre-define functions to be used **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, bmat\n",
    "from time import time\n",
    "from os.path import isfile\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# The csv processing functions are wrapped in helper.py\n",
    "from helper import csv_parse, write_submission\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMS(pred, true):\n",
    "    rms = np.sqrt(np.sum((pred-true)**2)/len(pred))\n",
    "    return rms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Read in the training data and pre-process the data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in processed csv_train file: Success!\n"
     ]
    }
   ],
   "source": [
    "# Loading the train data\n",
    "train_csv_raw = './cil-collab-filtering-2018/data_train.csv'\n",
    "csv_train = './data_train_post.csv'\n",
    "if isfile(csv_train):\n",
    "    print('Read in processed csv_train file: Success!')\n",
    "    df_train = pd.read_csv(csv_train)\n",
    "else:\n",
    "    df_train = csv_parse(train_csv_raw, csv_train)\n",
    "\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Creat a baseline solution \n",
    "\n",
    "We create a baseline solution by setting the missing values to the average over all observed ratings for a particular item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the size of the training data\n",
    "global user_N, item_N \n",
    "user_N = 10000\n",
    "item_N = 1000\n",
    "    \n",
    "# Prepare baseline matrix \n",
    "# Calculate mean rating for every single item\n",
    "mean_per_item = df_train.groupby('col_id')['Prediction'].mean().as_matrix()\n",
    "\n",
    "# Form A with sparse matrix (more efficient)\n",
    "A = coo_matrix((df_train['Prediction'], \n",
    "                (df_train['row_id']-1, df_train['col_id']-1))\n",
    "              ).todense()\n",
    "A = A + mean_per_item \n",
    "A[df_train['row_id']-1, df_train['col_id']-1] = df_train['Prediction']\n",
    "\n",
    "# write_submission(A, dst='./cil-collab-filtering-2018/submission_baseline.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: SVD Decomposition\n",
    "\n",
    "Compute the SVD Decompostion of the training matrix with the imputed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of userValue matrix: 10000 x 1000\n",
      "Shape of itemValue matrix: 1000 x 1000\n"
     ]
    }
   ],
   "source": [
    "# Perform SVD on matrix A\n",
    "u, s, vh = np.linalg.svd(A, full_matrices=False)\n",
    "s_diag = np.diag(np.sqrt(s))\n",
    "u_prime = np.dot(u, s_diag)\n",
    "vh_prime = np.dot(vh,s_diag)\n",
    "\n",
    "print('Shape of userValue matrix: %d x %d' % u_prime.shape)\n",
    "print('Shape of itemValue matrix: %d x %d' % vh_prime.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Model Selection **: Select a number k of eigenvalues to be used and truncate U and V accordingly. Evaluate the model performace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rms = []\n",
    "# k_range = np.arange(1, 201, 10)\n",
    "\n",
    "# for k in k_range:\n",
    "#     A_pred = np.dot(u_prime[:, 0:k], vh_prime[0:k, :])\n",
    "#     df_train['my_Prediction'] = A_pred[df_train['row_id']-1, df_train['col_id']-1].T\n",
    "#     rms.append(RMS(df_train['my_Prediction'], df_train['Prediction']))\n",
    "    \n",
    "# plt.plot(k_range, rms)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Step 3: Make Prediction \n",
    "\n",
    "Make prediction about the missing values with gradient descent algorithms\n",
    "\n",
    "** Algorithm 1: Stocastic Gradient Descent **\n",
    "\n",
    "The misfit function here is given as: \n",
    "$$ \\min_{q,p}\\left.\\{ \\sum_{u,i\\in\\kappa}{(r_{ui}-q_i^Tp_u)^2} + \\lambda(||q_i||^2 + ||p_u||^2)\\right.\\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randint, choice\n",
    "\n",
    "def misfit_func(r, p, q, user_ids, item_ids, lamda):\n",
    "    \n",
    "    global user_N, item_N\n",
    "    \n",
    "    r_pred = np.dot(q, p)  \n",
    "    err = (r.todense() - r_pred)[user_ids, item_ids]\n",
    "    loss = 0.5 * np.sqrt(np.sum(np.asarray(err) **2)/len(user_ids)) #+ lamda * (norm(userValue, axis=1)+norm(itemValue, axis=0)) \n",
    "    err_matrix = coo_matrix((np.asarray(err)[0], (user_ids, item_ids))).todense()\n",
    "    \n",
    "    return loss, err_matrix\n",
    "\n",
    "def SGD(r, p, q, user_ids, item_ids, gamma, max_iter, epsilon):\n",
    "    \n",
    "    global user_N, item_N\n",
    "    \n",
    "    sample = 100\n",
    "    trace = []\n",
    "    loss, err = misfit_func(r, p, q, user_ids, item_ids, lamda)\n",
    "    print('Initial Loss: %.5f' % loss)\n",
    "    print('*'*60)\n",
    "    i_iter = 1\n",
    "    tic = time()\n",
    "    while i_iter <= max_iter and loss >= epsilon:\n",
    "        loss_old = loss.copy()\n",
    "        rand_ids = randint(0, len(user_ids), sample)\n",
    "        for rand_id in rand_ids:\n",
    "            user, item = (user_ids[rand_id], item_ids[rand_id])\n",
    "            p[:, item] += gamma * (err[user, item] * q[user, :].T - lamda*p[:, item]) #np.dot(q.T, err[:, item])\n",
    "            q[user, :] += gamma * (err[user, item] * p[:, item].T - lamda*q[user, :]) #np.dot(err[user, :], p.T)\n",
    "        \n",
    "#         users, items = (user_ids[rand_ids], item_ids[rand_ids])\n",
    "#         err_sample = np.asarray(err[users, items])[0]\n",
    "# #         print(err_sample.shape)\n",
    "# #         print(q[users, :].T.shape)\n",
    "#         p[:, items] += gamma * ((err_sample * q[users, :]).T - lamda*p[:, items])\n",
    "#         q[users, :] += gamma * (err_sample * p[:, items].T - lamda*q[users, :]) \n",
    "            \n",
    "        loss, err = misfit_func(r, p, q, user_ids, item_ids, lamda)\n",
    "        \n",
    "#         if i_iter%20==0 and loss > loss_old \\\n",
    "#             and sample <= 200:\n",
    "#             sample += 5 \n",
    "            \n",
    "        trace.append([i_iter, loss])\n",
    "        if i_iter % 100 == 0:\n",
    "            toc = time()\n",
    "            print('Iteration: %d, Misfit: %.5f, Sample: %d' % (i_iter, loss, sample))\n",
    "            print('Average time per iteration: %.4f' % ((toc-tic)/i_iter))\n",
    "            print('*'*60)\n",
    "        \n",
    "        if np.abs(loss_old-loss) <= 1e-6:\n",
    "            break\n",
    "        else:\n",
    "            i_iter += 1 \n",
    "        \n",
    "    return p, q, np.asarray(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss: 1.90435\n",
      "************************************************************\n",
      "Iteration: 100, Misfit: 0.76413, Sample: 100\n",
      "Average time per iteration: 0.1928\n",
      "************************************************************\n",
      "Iteration: 200, Misfit: 0.68221, Sample: 100\n",
      "Average time per iteration: 0.1920\n",
      "************************************************************\n",
      "Iteration: 300, Misfit: 0.65518, Sample: 100\n",
      "Average time per iteration: 0.1916\n",
      "************************************************************\n",
      "Iteration: 400, Misfit: 0.64241, Sample: 100\n",
      "Average time per iteration: 0.1914\n",
      "************************************************************\n",
      "Iteration: 500, Misfit: 0.63549, Sample: 100\n",
      "Average time per iteration: 0.1917\n",
      "************************************************************\n",
      "Iteration: 600, Misfit: 0.62491, Sample: 100\n",
      "Average time per iteration: 0.1916\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "k_select = 10      # Turncation number \n",
    "gamma = 0.1        # Learning rate\n",
    "max_iter = 5000      # Maximum iteration\n",
    "misfit = []          # store the misfit value\n",
    "epsilon = 1e-3       # accept condition\n",
    "\n",
    "lamda=0.1\n",
    "\n",
    "p = vh_prime[0:k_select, :]\n",
    "q = u_prime[:, 0:k_select]\n",
    "\n",
    "# print(p.shape, q.shape)\n",
    "\n",
    "train_data = df_train['Prediction']\n",
    "user_ids = df_train['row_id'].values - 1\n",
    "item_ids = df_train['col_id'].values - 1\n",
    "\n",
    "r = coo_matrix((train_data, (user_ids, item_ids)))\n",
    "\n",
    "p, q, trace = SGD(r, p, q, user_ids, item_ids, gamma, max_iter, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trace[:, 0], trace[:,1])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('RMS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_pred = np.dot(q, p)\n",
    "write_submission(A_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
